{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129eb517-ba4d-40af-af57-7c1ef7fcdb32",
   "metadata": {},
   "source": [
    "# MAP5935 - Statistical Learning (Chapter 6 - Linear Model Selection and Regilarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed03b872-866a-49fb-9ed2-ee78a8b868db",
   "metadata": {},
   "source": [
    "**Prof. Christian Jäkel**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4017957-4852-4b1a-bb41-68a7c15e2eea",
   "metadata": {},
   "source": [
    "https://www.statlearning.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c423aba1-f997-447d-9bf2-ae565273983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ccd6b-cdb7-40a1-911c-53051730c3fc",
   "metadata": {},
   "source": [
    "## *Conceptual Exercises*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c21488-26e0-4c3a-9bd3-14671d1737ef",
   "metadata": {},
   "source": [
    "### (5) It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give quite different coefficient values to correlated variables. We will now explore this property in a very simple setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051fae3d-9ba8-403e-b78b-4c853f06dcb4",
   "metadata": {},
   "source": [
    "### Suppose that $n = 2$, $p = 2$, $x_{11} = x_{12}$, $x_{21} = x_{22}$. Furthermore, suppose that $y1+y2 = 0$ and $x_{11}+x_{21} = 0$ and $x_{12}+x_{22} = 0$, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: $\\hat\\beta_0=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed514c69-2f16-4b50-a632-d909dfc20d5a",
   "metadata": {},
   "source": [
    "### (a) Write out the ridge regression optimization problem in this setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57dca83-cf31-4c45-b2ae-76dd05daf58e",
   "metadata": {},
   "source": [
    "**Solution**: The regular *ridge regression* fitting procedure minimizes $L(\\boldsymbol{\\beta})$ such that:\n",
    "\n",
    "$$\n",
    "L(\\boldsymbol{\\beta}) = \\underbrace{\\sum_{i=1}^n\\Big(y_i-\\beta_0-\\sum_{j=1}^p\\beta_j x_{ij}\\Big)^2}_{RSS}+\\lambda\\sum_{j=1}^p\\beta_j^2\n",
    "$$\n",
    "\n",
    "Supposing that $n=2$, $p=2$, and\n",
    "$$\n",
    "x_{11}=x_{12},\\qquad x_{21}=x_{22}.\n",
    "$$\n",
    "... and that:\n",
    "$$\n",
    "y_1+y_2=0,\\qquad x_{11}+x_{21}=0,\\qquad x_{12}+x_{22}=0,\n",
    "$$\n",
    "... with the intercept in least squares, ridge, or lasso is zero: $\\hat\\beta_0=0$.\n",
    "\n",
    "We have *ridge regression* problem by solving the minimization problem considering the equation, as seen in this chapter:\n",
    "$$\n",
    "L(\\hat{\\boldsymbol{\\beta}})=\\min_{\\hat\\beta_1,\\hat\\beta_2}\\;\n",
    "\\sum_{i=1}^{2}\\Big(y_i - x_{i1}\\hat\\beta_1 - x_{i2}\\hat\\beta_2\\Big)^2\n",
    "\\;+\\;\\lambda\\left(\\hat\\beta_1^2+\\hat\\beta_2^2\\right).\n",
    "$$\n",
    "\n",
    "with $\\lambda$ being the regularization parameter.\n",
    "\n",
    "Using the facts $x_{11}=x_{12}$ and $x_{21}=x_{22}$, this can rewrite the optimization problem as follows:\n",
    "$$\n",
    "\\boxed{L(\\hat{\\boldsymbol{\\beta}})=\\min_{\\hat\\beta_1,\\hat\\beta_2}\\;\n",
    "\\Big(y_1 - x_{11}(\\hat\\beta_1+\\hat\\beta_2)\\Big)^2\n",
    "+\\Big(y_2 - x_{21}(\\hat\\beta_1+\\hat\\beta_2)\\Big)^2\n",
    "\\;+\\;\\lambda\\left(\\hat\\beta_1^2+\\hat\\beta_2^2\\right).}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83190717-85f7-4866-8f13-7d74718f8069",
   "metadata": {},
   "source": [
    "### (b) Argue that in this setting, the ridge coefficient estimates satisfy $\\hat\\beta_1=\\hat\\beta_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c6b49-5b07-4532-a3d5-018a30e31cb1",
   "metadata": {},
   "source": [
    "**Solution**: From part (a), with $x_{11}=x_{12}$ and $x_{21}=x_{22}$, the ridge objective is\n",
    "$$\n",
    "L(\\hat\\beta_1,\\hat\\beta_2)=\n",
    "\\big(y_1-x_{11}(\\hat\\beta_1+\\hat\\beta_2)\\big)^2+\n",
    "\\big(y_2-x_{21}(\\hat\\beta_1+\\hat\\beta_2)\\big)^2\n",
    "+\\lambda(\\hat\\beta_1^2+\\hat\\beta_2^2).\n",
    "$$\n",
    "\n",
    "The data-fit term depends only on the sum $s=\\hat\\beta_1+\\hat\\beta_2$.\n",
    "\n",
    "For any $(\\hat\\beta_1,\\hat\\beta_2)$, let $m=(\\hat\\beta_1+\\hat\\beta_2)/2$ and $d=(\\hat\\beta_1-\\hat\\beta_2)/2$.\n",
    "\n",
    "Replacing $(\\hat\\beta_1,\\hat\\beta_2)$ by $(m,m)$ keeps $s$ and the fit unchanged, but changes the penalty to\n",
    "$$\n",
    "\\hat\\beta_1^2+\\hat\\beta_2^2 \n",
    "= (m+d)^2+(m-d)^2.\n",
    "$$\n",
    "Expanding,\n",
    "$$\n",
    "= 2m^2 + 2d^2\n",
    "= 2m^2 + \\frac{(\\hat\\beta_1-\\hat\\beta_2)^2}{2}.\n",
    "$$\n",
    "\n",
    "Since $\\frac{(\\hat\\beta_1-\\hat\\beta_2)^2}{2}\\ge 0$, we have\n",
    "$$\n",
    "\\hat\\beta_1^2+\\hat\\beta_2^2 \\;\\ge\\;2m^2,\n",
    "$$\n",
    "\n",
    "and, when equality holds $\\implies\\hat\\beta_1=\\hat\\beta_2$.\n",
    "\n",
    "Thus, the ridge penalty is minimized when $\\hat\\beta_1=\\hat\\beta_2$.  \n",
    "\n",
    "Since the loss part depends only on $s$, the overall minimizer must satisfy\n",
    "$$\n",
    "\\boxed{\\hat\\beta_1=\\hat\\beta_2}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f92bf7c-d988-4564-8eb3-3eaae944c4e6",
   "metadata": {},
   "source": [
    "### (c) Write out the lasso optimization problem in this setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9fd589-097e-426e-8ff0-bac599561169",
   "metadata": {},
   "source": [
    "**Solution**: The regular *lasso* fitting procedure minimizes $L(\\boldsymbol{\\beta})$ such that:\n",
    "\n",
    "$$\n",
    "L(\\boldsymbol{\\beta}) = \\underbrace{\\sum_{i=1}^n\\Big(y_i-\\beta_0-\\sum_{j=1}^p\\beta_j x_{ij}\\Big)^2}_{RSS}+\\lambda\\sum_{j=1}^p|\\beta_j|\n",
    "$$\n",
    "\n",
    "With $\\hat\\beta_0=0$, $x_{11}=x_{12}$, and $x_{21}=x_{22}$, the *lasso objective* is\n",
    "$$\n",
    "L(\\boldsymbol{\\beta}) = \\min_{\\beta_1,\\beta_2}\\;\n",
    "\\sum_{i=1}^{2}\\Big(y_i - x_{i1}\\beta_1 - x_{i2}\\beta_2\\Big)^2\n",
    "\\;+\\;\\lambda\\big(|\\beta_1|+|\\beta_2|\\big).\n",
    "$$\n",
    "\n",
    "Using $x_{11}=x_{12}$ and $x_{21}=x_{22}$, this can be written equivalently as\n",
    "$$\n",
    "\\boxed{L(\\boldsymbol{\\beta}) = \\min_{\\beta_1,\\beta_2}\\;\n",
    "\\Big(y_1 - x_{11}(\\beta_1+\\beta_2)\\Big)^2\n",
    "+\\Big(y_2 - x_{21}(\\beta_1+\\beta_2)\\Big)^2\n",
    "\\;+\\;\\lambda\\big(|\\beta_1|+|\\beta_2|\\big).}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1577e60-e98b-44f6-9167-4d1b6c5e5de6",
   "metadata": {},
   "source": [
    "### (d) Argue that in this setting, the lasso coefficients $\\hat\\beta_1$ and $\\hat\\beta_2$ are not unique—in other words, there are many possible solutions to the optimization problem in (c). Describe these solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3051240-831d-4a13-af10-a2243f15e130",
   "metadata": {},
   "source": [
    "**Solution**: Since $x_{11}=x_{12}$ and $x_{21}=x_{22}$, the data-fit depends only on the sum $s=\\beta_1+\\beta_2$.  \n",
    "\n",
    "For fixed $s$, we must minimize $|\\beta_1|+|\\beta_2|$ subject to $\\beta_1+\\beta_2=s$.  \n",
    "\n",
    "By the triangle inequality (see reference below),\n",
    "$$\n",
    "|\\beta_1|+|\\beta_2| \\;\\ge\\; |\\beta_1+\\beta_2| \\;=\\; |s|,\n",
    "$$\n",
    "with equality **iff** $\\beta_1$ and $\\beta_2$ have the **same sign** (zeros allowed).  \n",
    "\n",
    "Therefore the lasso reduces to the **univariate** problem\n",
    "$$\n",
    "s^\\star = \\min_{s}\\;\\text{RSS}(s)+\\lambda|s|,\n",
    "$$\n",
    "which has a unique minimizer $s^\\star$.  \n",
    "\n",
    "Every pair $(\\hat\\beta_1,\\hat\\beta_2)$ with\n",
    "$$\n",
    "\\hat\\beta_1+\\hat\\beta_2 = s^\\star\n",
    "\\quad\\text{and}\\quad\n",
    "\\operatorname{sign}(\\hat\\beta_1)=\\operatorname{sign}(\\hat\\beta_2)=\\operatorname{sign}(s^\\star)\n",
    "$$\n",
    "achieves the same objective value.\n",
    "\n",
    "When\n",
    "\n",
    "$$\n",
    "\\hat\\beta_1=t\\implies \\hat\\beta_2=s^\\star-t\n",
    "$$\n",
    "\n",
    "Hence, if $s^\\star\\neq 0$, there is a **set of solutions**:\n",
    "\n",
    "$$\n",
    "\\mathbb{S}=\\{s^\\star\\text{ such that }(\\hat\\beta_1,\\hat\\beta_2)=(t,\\,s^\\star-t):\n",
    "\\begin{cases}\n",
    "\\; t\\in[0,s^\\star]\\}\\quad\\text{if }s^\\star\\ge 0\\}\\\\\n",
    "\\; t\\in[s^\\star,0]\\}\\quad\\text{if }s^\\star\\le 0\\}.\n",
    "\\end{cases}\n",
    "$$\n",
    "  \n",
    "(If $s^\\star=0$, the unique solution is $(0,0)$.) $\\square$\n",
    "\n",
    "#### References\n",
    "- Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press (page 634)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e330d5-54b4-4a17-9896-eb3f98c8ea39",
   "metadata": {},
   "source": [
    "## *Applied Exercises*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b45c67e-646b-497a-a56e-04a6b2025b81",
   "metadata": {},
   "source": [
    "### (9) In this exercise, we will predict the number of applications received (`Apps`) using the other variables in the `College` data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f46c42-93d1-44e6-898e-108ec3cd7918",
   "metadata": {},
   "source": [
    "### (a) Split the data set into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d2a5f0-dd98-4525-8850-f20d5ba9ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "College = pd.read_csv(\"../Data/College.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d78bc6fd-0791-4089-9cef-f6cba8475a05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 777 entries, 0 to 776\n",
      "Data columns (total 19 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Unnamed: 0   777 non-null    object \n",
      " 1   Private      777 non-null    object \n",
      " 2   Apps         777 non-null    int64  \n",
      " 3   Accept       777 non-null    int64  \n",
      " 4   Enroll       777 non-null    int64  \n",
      " 5   Top10perc    777 non-null    int64  \n",
      " 6   Top25perc    777 non-null    int64  \n",
      " 7   F.Undergrad  777 non-null    int64  \n",
      " 8   P.Undergrad  777 non-null    int64  \n",
      " 9   Outstate     777 non-null    int64  \n",
      " 10  Room.Board   777 non-null    int64  \n",
      " 11  Books        777 non-null    int64  \n",
      " 12  Personal     777 non-null    int64  \n",
      " 13  PhD          777 non-null    int64  \n",
      " 14  Terminal     777 non-null    int64  \n",
      " 15  S.F.Ratio    777 non-null    float64\n",
      " 16  perc.alumni  777 non-null    int64  \n",
      " 17  Expend       777 non-null    int64  \n",
      " 18  Grad.Rate    777 non-null    int64  \n",
      "dtypes: float64(1), int64(16), object(2)\n",
      "memory usage: 115.5+ KB\n"
     ]
    }
   ],
   "source": [
    "College.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e24c4e79-ca00-4266-a292-b431fc5a0c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Private</th>\n",
       "      <th>Apps</th>\n",
       "      <th>Accept</th>\n",
       "      <th>Enroll</th>\n",
       "      <th>Top10perc</th>\n",
       "      <th>Top25perc</th>\n",
       "      <th>F.Undergrad</th>\n",
       "      <th>P.Undergrad</th>\n",
       "      <th>Outstate</th>\n",
       "      <th>Room.Board</th>\n",
       "      <th>Books</th>\n",
       "      <th>Personal</th>\n",
       "      <th>PhD</th>\n",
       "      <th>Terminal</th>\n",
       "      <th>S.F.Ratio</th>\n",
       "      <th>perc.alumni</th>\n",
       "      <th>Expend</th>\n",
       "      <th>Grad.Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilene Christian University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1660</td>\n",
       "      <td>1232</td>\n",
       "      <td>721</td>\n",
       "      <td>23</td>\n",
       "      <td>52</td>\n",
       "      <td>2885</td>\n",
       "      <td>537</td>\n",
       "      <td>7440</td>\n",
       "      <td>3300</td>\n",
       "      <td>450</td>\n",
       "      <td>2200</td>\n",
       "      <td>70</td>\n",
       "      <td>78</td>\n",
       "      <td>18.1</td>\n",
       "      <td>12</td>\n",
       "      <td>7041</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelphi University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2186</td>\n",
       "      <td>1924</td>\n",
       "      <td>512</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>2683</td>\n",
       "      <td>1227</td>\n",
       "      <td>12280</td>\n",
       "      <td>6450</td>\n",
       "      <td>750</td>\n",
       "      <td>1500</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16</td>\n",
       "      <td>10527</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adrian College</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1428</td>\n",
       "      <td>1097</td>\n",
       "      <td>336</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "      <td>1036</td>\n",
       "      <td>99</td>\n",
       "      <td>11250</td>\n",
       "      <td>3750</td>\n",
       "      <td>400</td>\n",
       "      <td>1165</td>\n",
       "      <td>53</td>\n",
       "      <td>66</td>\n",
       "      <td>12.9</td>\n",
       "      <td>30</td>\n",
       "      <td>8735</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agnes Scott College</td>\n",
       "      <td>Yes</td>\n",
       "      <td>417</td>\n",
       "      <td>349</td>\n",
       "      <td>137</td>\n",
       "      <td>60</td>\n",
       "      <td>89</td>\n",
       "      <td>510</td>\n",
       "      <td>63</td>\n",
       "      <td>12960</td>\n",
       "      <td>5450</td>\n",
       "      <td>450</td>\n",
       "      <td>875</td>\n",
       "      <td>92</td>\n",
       "      <td>97</td>\n",
       "      <td>7.7</td>\n",
       "      <td>37</td>\n",
       "      <td>19016</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alaska Pacific University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>193</td>\n",
       "      <td>146</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>44</td>\n",
       "      <td>249</td>\n",
       "      <td>869</td>\n",
       "      <td>7560</td>\n",
       "      <td>4120</td>\n",
       "      <td>800</td>\n",
       "      <td>1500</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>11.9</td>\n",
       "      <td>2</td>\n",
       "      <td>10922</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Unnamed: 0 Private  Apps  Accept  Enroll  Top10perc  \\\n",
       "0  Abilene Christian University     Yes  1660    1232     721         23   \n",
       "1            Adelphi University     Yes  2186    1924     512         16   \n",
       "2                Adrian College     Yes  1428    1097     336         22   \n",
       "3           Agnes Scott College     Yes   417     349     137         60   \n",
       "4     Alaska Pacific University     Yes   193     146      55         16   \n",
       "\n",
       "   Top25perc  F.Undergrad  P.Undergrad  Outstate  Room.Board  Books  Personal  \\\n",
       "0         52         2885          537      7440        3300    450      2200   \n",
       "1         29         2683         1227     12280        6450    750      1500   \n",
       "2         50         1036           99     11250        3750    400      1165   \n",
       "3         89          510           63     12960        5450    450       875   \n",
       "4         44          249          869      7560        4120    800      1500   \n",
       "\n",
       "   PhD  Terminal  S.F.Ratio  perc.alumni  Expend  Grad.Rate  \n",
       "0   70        78       18.1           12    7041         60  \n",
       "1   29        30       12.2           16   10527         56  \n",
       "2   53        66       12.9           30    8735         54  \n",
       "3   92        97        7.7           37   19016         59  \n",
       "4   76        72       11.9            2   10922         15  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "College.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf908e9-f797-455d-99f9-6e33418b5f9e",
   "metadata": {},
   "source": [
    "We’ll fit a logistic regression using **income** and **balance** to predict **default**.\n",
    "The response needs to be binary, so we map `\"Yes\"→1`, `\"No\"→0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4647fb3-ccd6-4cbc-8261-4008ab038e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  X_train: (621, 17)   y_train: (621,)\n",
      "  X_test : (156, 17)   y_test : (156,)\n",
      "\n",
      "Private proportion (train):\n",
      "Private\n",
      "Yes    0.728\n",
      "No     0.272\n",
      "\n",
      "Private proportion (test):\n",
      "Private\n",
      "Yes    0.724\n",
      "No     0.276\n"
     ]
    }
   ],
   "source": [
    "# Get a copy of the DataFrame\n",
    "df = College.copy()\n",
    "\n",
    "# Drop non-informative ID column if present\n",
    "if \"Unnamed: 0\" in df.columns:\n",
    "    df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "# Define target and features\n",
    "y = df[\"Apps\"]\n",
    "X = df.drop(columns=[\"Apps\"])\n",
    "\n",
    "# Optional stratification by the `Private` flag (helps keep balance)\n",
    "stratifier = X[\"Private\"] if \"Private\" in X.columns else None\n",
    "\n",
    "# Split: 80% train / 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=stratifier\n",
    ")\n",
    "\n",
    "# Quick checks\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_train:\", X_train.shape, \"  y_train:\", y_train.shape)\n",
    "print(\"  X_test :\", X_test.shape,  \"  y_test :\", y_test.shape)\n",
    "\n",
    "if \"Private\" in X_train.columns:\n",
    "    print(\"\\nPrivate proportion (train):\")\n",
    "    print(X_train[\"Private\"].value_counts(normalize=True).round(3).to_string())\n",
    "    print(\"\\nPrivate proportion (test):\")\n",
    "    print(X_test[\"Private\"].value_counts(normalize=True).round(3).to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eca8d5-bc09-48e7-8e53-3865902afc59",
   "metadata": {},
   "source": [
    "### (b) Fit a linear model using least squares on the training set, and report the test error obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3002df08-8534-422c-80fb-9f36827d2af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Model:\n",
      "-------------\n",
      "Test MSE : 1,085,881.76\n",
      "Test RMSE: 1,042.06\n",
      "Test MAE : 646.07\n",
      "Test R^2 : 0.912\n"
     ]
    }
   ],
   "source": [
    "# Reuse X_train, X_test, y_train, y_test created in part (a)\n",
    "\n",
    "# Preprocessor: one-hot encode object columns; passthrough numerics\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"),\n",
    "         selector(dtype_include=object)),\n",
    "        (\"num\", \"passthrough\",\n",
    "         selector(dtype_include=[\"int64\", \"float64\"]))\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Pipeline: preprocessing + OLS\n",
    "ols = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit on training data\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = ols.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "test_mse  = mean_squared_error(y_test, y_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2   = r2_score(y_test, y_pred)\n",
    "test_mae  = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print('\\nLinear Model:\\n-------------')\n",
    "print(f\"Test MSE : {test_mse:,.2f}\")\n",
    "print(f\"Test RMSE: {test_rmse:,.2f}\")\n",
    "print(f\"Test MAE : {test_mae:,.2f}\")\n",
    "print(f\"Test R^2 : {test_r2:,.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28383c-ff9d-48ff-aba6-f55c96f0351f",
   "metadata": {},
   "source": [
    "### (c) Fit a ridge regression model on the training set, with $\\lambda$ chosen by cross-validation. Report the test error obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b882cc1-de70-450d-8080-760f04426556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge Regression:\n",
      "-----------------\n",
      "Selected lambda (alpha): 0.0001\n",
      "Test MSE : 1,085,880.52\n",
      "Test RMSE: 1,042.06\n",
      "Test MAE : 646.07\n",
      "Test R^2 : 0.912\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-4, 4, 81)\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "ridge_cv = RidgeCV(\n",
    "    alphas=alphas,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_squared_error\"  # keep your metric\n",
    ")\n",
    "\n",
    "ridge = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"scale\", StandardScaler(with_mean=False)),\n",
    "    (\"model\", ridge_cv)\n",
    "])\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "best_alpha = ridge.named_steps[\"model\"].alpha_\n",
    "y_pred = ridge.predict(X_test)\n",
    "\n",
    "test_mse  = mean_squared_error(y_test, y_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2   = r2_score(y_test, y_pred)\n",
    "test_mae  = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print('\\nRidge Regression:\\n-----------------')\n",
    "print(f\"Selected lambda (alpha): {best_alpha:.6g}\")\n",
    "print(f\"Test MSE : {test_mse:,.2f}\")\n",
    "print(f\"Test RMSE: {test_rmse:,.2f}\")\n",
    "print(f\"Test MAE : {test_mae:,.2f}\")\n",
    "print(f\"Test R^2 : {test_r2:,.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efba831-ead3-49d3-a00a-7d8b9634c4d8",
   "metadata": {},
   "source": [
    "### (d) Fit a lasso model on the training set, with $\\lambda$ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a30dcf25-7617-43ec-b882-6ce102165e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso Regression:\n",
      "-----------------\n",
      "Selected lambda (alpha): 10\n",
      "Test MSE : 1,058,263.23\n",
      "Test RMSE: 1,028.72\n",
      "Test MAE : 630.88\n",
      "Test R^2 : 0.915\n",
      "Non-zero coefficients: 14 out of 17\n"
     ]
    }
   ],
   "source": [
    "# Alpha (lambda) grid on log-scale (covers near-OLS to strong shrinkage)\n",
    "alphas = np.logspace(-4, 2, 61)  # 1e-4 … 1e2\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "lasso_cv = LassoCV(\n",
    "    alphas=alphas,\n",
    "    cv=cv,\n",
    "    max_iter=20000,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,   # for reproducibility\n",
    ")\n",
    "\n",
    "lasso = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"scale\", StandardScaler(with_mean=True)),\n",
    "    (\"model\", lasso_cv)\n",
    "])\n",
    "\n",
    "# Fit\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Selected lambda (alpha)\n",
    "best_alpha = lasso.named_steps[\"model\"].alpha_\n",
    "\n",
    "# Predict on test\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "test_mse  = mean_squared_error(y_test, y_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2   = r2_score(y_test, y_pred)\n",
    "test_mae  = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Sparsity: number of non-zero coefficients (tolerate tiny numerical noise)\n",
    "coefs = lasso.named_steps[\"model\"].coef_\n",
    "nnz = int(np.sum(np.abs(coefs) > 1e-8))\n",
    "p_total = coefs.size\n",
    "\n",
    "print('\\nLasso Regression:\\n-----------------')\n",
    "print(f\"Selected lambda (alpha): {best_alpha:.6g}\")\n",
    "print(f\"Test MSE : {test_mse:,.2f}\")\n",
    "print(f\"Test RMSE: {test_rmse:,.2f}\")\n",
    "print(f\"Test MAE : {test_mae:,.2f}\")\n",
    "print(f\"Test R^2 : {test_r2:,.3f}\")\n",
    "print(f\"Non-zero coefficients: {nnz} out of {p_total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2bd915-461a-4d6d-b137-f61718826b6a",
   "metadata": {},
   "source": [
    "### (e) Fit a PCR model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of $M$ selected by cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b991bafc-92f4-4a15-86a8-80d3e24e6f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCR Model:\n",
      "---------\n",
      "Total encoded predictors (p): 17\n",
      "Selected number of PCs (M): 17\n",
      "Test MSE : 1,085,881.76\n",
      "Test RMSE: 1,042.06\n",
      "Test MAE : 646.07\n",
      "Test R^2 : 0.912\n"
     ]
    }
   ],
   "source": [
    "## Standardizing X_train\n",
    "Xtr_proc = preprocess.fit_transform(X_train)\n",
    "p_total = Xtr_proc.shape[1]\n",
    "\n",
    "# Build PCR pipeline: preprocess -> scale -> PCA(M) -> OLS\n",
    "pcr = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"scale\", StandardScaler(with_mean=True)),\n",
    "    (\"pca\", PCA(svd_solver=\"full\")),   # deterministic for given data\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# CV over M = 1..p_total\n",
    "M_list = list(range(1, p_total + 1))\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Exhaustive search over specified parameter values for PCR\n",
    "grid_pcr = GridSearchCV(\n",
    "    estimator=pcr,\n",
    "    param_grid={\"pca__n_components\": M_list},\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit on training\n",
    "grid_pcr.fit(X_train, y_train)\n",
    "\n",
    "best_M = grid_pcr.best_params_[\"pca__n_components\"]\n",
    "best_pcr = grid_pcr.best_estimator_\n",
    "\n",
    "# Evaluate on test\n",
    "y_pred = best_pcr.predict(X_test)\n",
    "\n",
    "test_mse  = mean_squared_error(y_test, y_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2   = r2_score(y_test, y_pred)\n",
    "test_mae  = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print('\\nPCR Model:\\n---------')\n",
    "print(f\"Total encoded predictors (p): {p_total}\")\n",
    "print(f\"Selected number of PCs (M): {best_M}\")\n",
    "print(f\"Test MSE : {test_mse:,.2f}\")\n",
    "print(f\"Test RMSE: {test_rmse:,.2f}\")\n",
    "print(f\"Test MAE : {test_mae:,.2f}\")\n",
    "print(f\"Test R^2 : {test_r2:,.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dab26c-0916-4c63-88c0-64bf18fb4bd8",
   "metadata": {},
   "source": [
    "### (f) Fit a PLS model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of $M$ selected by cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9b0990f-8cba-48ed-95cb-198aa0272d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLS Model:\n",
      "---------\n",
      "Total encoded predictors (p): 17\n",
      "Selected number of components (M): 17\n",
      "Test MSE : 1,085,881.76\n",
      "Test RMSE: 1,042.06\n",
      "Test MAE : 646.07\n",
      "Test R^2 : 0.912\n"
     ]
    }
   ],
   "source": [
    "# Build PLS pipeline: preprocess -> scale -> PLS(M)\n",
    "#    We scale here, so set PLS(scale=False) to avoid double-scaling.\n",
    "pls_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"scale\", StandardScaler(with_mean=True)),\n",
    "    (\"pls\", PLSRegression(scale=False))\n",
    "])\n",
    "\n",
    "# CV over M = 1..p_total\n",
    "M_list = list(range(1, p_total + 1))\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "grid_pls = GridSearchCV(\n",
    "    estimator=pls_pipe,\n",
    "    param_grid={\"pls__n_components\": M_list},\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on training\n",
    "grid_pls.fit(X_train, y_train)\n",
    "\n",
    "best_M = grid_pls.best_params_[\"pls__n_components\"]\n",
    "best_pls = grid_pls.best_estimator_\n",
    "\n",
    "# Evaluate on test\n",
    "y_pred = best_pls.predict(X_test).ravel()  # PLS returns (n, 1); flatten to (n,)\n",
    "\n",
    "test_mse  = mean_squared_error(y_test, y_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2   = r2_score(y_test, y_pred)\n",
    "test_mae  = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"PLS Model:\")\n",
    "print(\"---------\")\n",
    "print(f\"Total encoded predictors (p): {p_total}\")\n",
    "print(f\"Selected number of components (M): {best_M}\")\n",
    "print(f\"Test MSE : {test_mse:,.2f}\")\n",
    "print(f\"Test RMSE: {test_rmse:,.2f}\")\n",
    "print(f\"Test MAE : {test_mae:,.2f}\")\n",
    "print(f\"Test R^2 : {test_r2:,.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70893733-6db2-43e7-8b22-f5ace0666879",
   "metadata": {},
   "source": [
    "### (g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babb0b0a-e618-430c-b45f-f1416a405312",
   "metadata": {},
   "source": [
    "**Accuracy of predictions**  \n",
    "- All methods achieve **high accuracy**: $R^2 \\approx 0.91$, meaning ~91% of the variance in `Apps` is explained by the predictors.  \n",
    "\n",
    "**Differences among methods**  \n",
    "- **OLS, Ridge, PCR, and PLS** all yield **essentially the same** test error (MSE ≈ 1,085,882; RMSE ≈ 1,042; $R^2 \\approx 0.912$).  \n",
    "  - Ridge chose a **tiny** $\\lambda$ (1e-4) → effectively **OLS**.  \n",
    "  - **PCR/PLS** selected **$M=p=17$** → they replicate the **OLS** fit.\n",
    "- **Lasso** is slightly better: **MSE 1,058,263; RMSE 1,028.7; MAE 630.9; $R^2=0.915$** with **14/17** non-zero coefficients.  \n",
    "  - Gains vs OLS: **MSE ↓ ~2.54%**, **RMSE ↓ ~1.28%**, **MAE ↓ ~2.35%**, $R^2= +0.003$.  \n",
    "\n",
    "**Comments**  \n",
    "- You can predict `Apps` (the number of applications received) **quite well** with these features.  \n",
    "- **No large differences** among methods for this split; **Lasso** offers a **small** improvement and a **simpler** model (14/17 coefficients)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
