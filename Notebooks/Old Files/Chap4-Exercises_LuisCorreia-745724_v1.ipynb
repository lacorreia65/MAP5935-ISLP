{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129eb517-ba4d-40af-af57-7c1ef7fcdb32",
   "metadata": {},
   "source": [
    "# MAP5935 - Statistical Learning (Chapter 4 - Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed03b872-866a-49fb-9ed2-ee78a8b868db",
   "metadata": {},
   "source": [
    "**Prof. Christian Jäkel**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4017957-4852-4b1a-bb41-68a7c15e2eea",
   "metadata": {},
   "source": [
    "https://www.statlearning.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358be43-a140-444c-8f5b-ab5cb6010de8",
   "metadata": {},
   "source": [
    "## *The Stock Market Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c423aba1-f997-447d-9bf2-ae565273983e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lacor\\anaconda3\\envs\\islp\\Lib\\site-packages\\scipy\\__init__.py:145: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 2.0.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: Não foi possível encontrar o módulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing _multiarray_umath: Não foi possível encontrar o módulo especificado."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m subplots\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msm\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mISLP\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_data\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mISLP\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (ModelSpec \u001b[38;5;28;01mas\u001b[39;00m MS, summarize)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\islp\\Lib\\site-packages\\statsmodels\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpatsy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m monkey_patch_cat_dtype\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__, __version_tuple__\n\u001b[32m      5\u001b[39m __version_info__ = __version_tuple__\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\islp\\Lib\\site-packages\\statsmodels\\compat\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_testing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PytestTester\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     asunicode,\n\u001b[32m      5\u001b[39m     asbytes,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     lfilter,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m __all__ = [\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33masunicode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33masbytes\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\islp\\Lib\\site-packages\\statsmodels\\tools\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_constant, categorical\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_testing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PytestTester\n\u001b[32m      4\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33madd_constant\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcategorical\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\islp\\Lib\\site-packages\\statsmodels\\tools\\tools.py:6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_using_pandas\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_like\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\islp\\Lib\\site-packages\\scipy\\linalg\\__init__.py:206\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m====================================\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mLinear algebra (:mod:`scipy.linalg`)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    203\u001b[39m \n\u001b[32m    204\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_misc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_cythonized_array_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_basic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\islp\\Lib\\site-packages\\scipy\\linalg\\_misc.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinAlgError\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mblas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlapack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[32m      6\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mLinAlgError\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLinAlgWarning\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnorm\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\islp\\Lib\\site-packages\\scipy\\linalg\\blas.py:213\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_np\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctools\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _fblas\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _cblas\n",
      "\u001b[31mImportError\u001b[39m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS, summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ceb3529-14c9-4c99-aaa2-b26ec82f3a3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: Não foi possível encontrar o módulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing _multiarray_umath: Não foi possível encontrar o módulo especificado."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mISLP\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_table\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mISLP\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m contrast\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdiscriminant_analysis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \\\n\u001b[32m      4\u001b[39m (LinearDiscriminantAnalysis \u001b[38;5;28;01mas\u001b[39;00m LDA ,\n\u001b[32m      5\u001b[39m QuadraticDiscriminantAnalysis \u001b[38;5;28;01mas\u001b[39;00m QDA)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\islp\\Lib\\site-packages\\ISLP\\__init__.py:12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresources\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (as_file,\n\u001b[32m     10\u001b[39m                                  files)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix \u001b[38;5;28;01mas\u001b[39;00m _confusion_matrix\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_classification\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unique_labels\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# data originally saved via: [sm.datasets.get_rdataset(n, 'ISLR').data.to_csv('../ISLP/data/%s.csv' % n, index=False) for n in ['Carseats', 'College', 'Credit', 'Default', 'Hitters', 'Auto', 'OJ', 'Portfolio', 'Smarket', 'Wage', 'Weekly', 'Caravan']]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\islp\\Lib\\site-packages\\sklearn\\__init__.py:82\u001b[39m\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m     \u001b[38;5;66;03m# in utils.\u001b[39;00m\n\u001b[32m     80\u001b[39m     \u001b[38;5;66;03m# (OpenMP is loaded by importing show_versions right after this block)\u001b[39;00m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mthreadpoolctl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadpoolController\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     86\u001b[39m     \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\islp\\Lib\\site-packages\\scipy\\linalg\\__init__.py:206\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m====================================\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mLinear algebra (:mod:`scipy.linalg`)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    203\u001b[39m \n\u001b[32m    204\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_misc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_cythonized_array_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_basic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\islp\\Lib\\site-packages\\scipy\\linalg\\_misc.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinAlgError\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mblas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlapack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[32m      6\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mLinAlgError\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLinAlgWarning\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnorm\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\islp\\Lib\\site-packages\\scipy\\linalg\\blas.py:213\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_np\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctools\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _fblas\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _cblas\n",
      "\u001b[31mImportError\u001b[39m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "from ISLP import confusion_table\n",
    "from ISLP.models import contrast\n",
    "from sklearn.discriminant_analysis import \\\n",
    "(LinearDiscriminantAnalysis as LDA ,\n",
    "QuadraticDiscriminantAnalysis as QDA)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12cad88b-e708-461a-9d62-a52adcb16c3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m Smarket = \u001b[43mload_data\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mSmarket\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m Smarket\n",
      "\u001b[31mNameError\u001b[39m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "Smarket = load_data('Smarket')\n",
    "Smarket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e4575-7da9-4ac2-bd07-7a70b9c5f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Smarket.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0101d1f6-7211-4423-bff9-8d85b5758df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Smarket.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb09b3d-eee5-4c7d-9cdc-cbde6c198a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrvars = Smarket.columns.drop(['Today', 'Direction'])\n",
    "Smarket[corrvars].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f38d3f-b148-40fb-8e09-506ab740eff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Smarket.plot(y='Volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae3919-0c88-4c61-bf0f-5a383c3f14a9",
   "metadata": {},
   "source": [
    "## *Logistic Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c5e14-13c5-4a83-97b8-de707d33c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "allvars = Smarket.columns.drop(['Today', 'Direction', 'Year'])\n",
    "\n",
    "# Model Specification\n",
    "design = MS(allvars)\n",
    "\n",
    "# Model Fit\n",
    "X = design.fit_transform(Smarket)\n",
    "y = Smarket.Direction == 'Up'\n",
    "glm = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "\n",
    "# Model Results\n",
    "results = glm.fit()\n",
    "summarize(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92ff062-834e-4000-8c18-241b9dcfa1c8",
   "metadata": {},
   "source": [
    "- None of *lags* are statistically signiticant in explaining the `Direction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86302d6e-1b03-44e2-adce-e9604e434105",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('params>')\n",
    "print(results.params)\n",
    "print('\\np-values>')\n",
    "print(results.pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2804ae-ab20-4e1d-b9b8-69dee202b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability that the market will go up, given values of the predictors\n",
    "probs = results.predict()\n",
    "probs [:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e338db-88c7-4511-8cc4-bf922084d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(['Down']*1250)\n",
    "labels[probs >0.5] = \"Up\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be7c914-77a9-4104-a3ee-7e97eb2106fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_table(labels , Smarket.Direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd4f02-eadd-443f-81e9-cb94ffe7f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (Smarket.Year < 2005)\n",
    "Smarket_train = Smarket.loc[train]\n",
    "Smarket_test = Smarket.loc[~train]\n",
    "Smarket_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1bf89-e387-45d9-b0ef-47c4a4a85fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model using only the subset of the observations that correspond to dates before 2005\n",
    "X_train , X_test = X.loc[train], X.loc[~train]\n",
    "y_train , y_test = y.loc[train], y.loc[~train]\n",
    "\n",
    "glm_train = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "\n",
    "results = glm_train.fit()\n",
    "probs = results.predict(exog=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f36d53-c484-4403-a5c3-cdfb90af41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Smarket.Direction\n",
    "L_train , L_test = D.loc[train], D.loc[~train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d06d9f-8459-479f-9ab7-cbff0f4792f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(['Down']*252)\n",
    "labels[probs > 0.5] = 'Up'\n",
    "confusion_table(labels , L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b4336-a6b0-4db8-9dde-c203433fa5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(labels == L_test), np.mean(labels != L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707eb11b-fe07-4c53-b00e-d506340028fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using variables more related with Direction\n",
    "model = MS(['Lag1', 'Lag2']).fit(Smarket)\n",
    "X = model.transform(Smarket)\n",
    "X_train , X_test = X.loc[train], X.loc[~train]\n",
    "glm_train = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "results = glm_train.fit()\n",
    "probs = results.predict(exog=X_test)\n",
    "labels = np.array(['Down']*252)\n",
    "labels[probs >0.5] = 'Up'\n",
    "confusion_table(labels , L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409b323-3620-453a-89be-2a7dac9d45ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Overall Accuracy')\n",
    "(35+106) /252 ,106/(106+76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa2cf6c-86e8-4e3e-abc9-c845e92df9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = pd.DataFrame({'Lag1':[1.2, 1.5], 'Lag2':[1.1, -0.8]})\n",
    "newX = model.transform(newdata)\n",
    "results.predict(newX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024fc80a-7d37-40f0-8207-49706af93daa",
   "metadata": {},
   "source": [
    "## *Linear Disctiminant Analysis (LDA)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25106039-0a27-41a4-a70b-8c371f68b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(store_covariance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68361f8f-e365-4373-b2b6-f9ead3c3b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test = [M.drop(columns=['intercept']) for M in [X_train , X_test]]\n",
    "lda.fit(X_train , L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fba1c2c-f119-486d-9dab-13feb4c21074",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bebb70-d160-4f64-84c7-f7851ecfda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee894d-6155-49f7-9d8a-2d71d147a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.priors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208de7c-db65-4fda-a1cf-db7189562752",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.scalings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d11f5-642e-451d-93cf-43c7038b54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_pred = lda.predict(X_test)\n",
    "confusion_table(lda_pred , L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c0e5d-cc1b-41cf-89bb-e926c42f3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_prob = lda.predict_proba(X_test)\n",
    "np.all(np.where(lda_prob[:,1] >= 0.5, 'Up','Down') == lda_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57abee26-0634-4649-b878-f944826f9dc1",
   "metadata": {},
   "source": [
    "## *Quadratic Disctiminant Analysis (QDA)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fb2261-5f18-4d74-97db-b5bfa7fa6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "qda = QDA(store_covariance=True)\n",
    "qda.fit(X_train , L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa1615d-addf-45cb-8568-1aaa9c627350",
   "metadata": {},
   "outputs": [],
   "source": [
    "qda.means_ , qda.priors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cbd73a-c67d-4d5b-8db1-d6b82511a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Class covariance\n",
    "qda.covariance_ [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed73730-f7a9-4175-973f-429fd4983fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_pred = qda.predict(X_test)\n",
    "confusion_table(qda_pred , L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eee8e8-6d99-45ed-8114-b379cce5253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Percentage of cases predictect accurately = {np.mean(qda_pred == L_test)*100.0:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec3b8b-4f89-41b8-8e2b-c8ace67e444d",
   "metadata": {},
   "source": [
    "## *Naive Bayes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a46c20-7ea3-4698-bb0b-22764ba48514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful with no assuptioon Normal Distribution, small number of observations\n",
    "NB = GaussianNB()\n",
    "NB.fit(X_train , L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf17c5a-795b-447a-8462-453c31b95ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf90e7-e04e-4d8b-861c-5384c9b10f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB.class_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5b46a-ea6d-4b56-8fe9-4683e8b2a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Theta=\\n{NB.theta_}')\n",
    "print(f'\\nVariance=\\n{NB.var_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed88f6f-3ecf-4fa3-90a5-1666aedd8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[L_train == 'Down'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700255f-4b99-4c4f-9f55-9c83a2d13247",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[L_train == 'Down'].var(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b6dd1-3b29-48c1-8e9b-61b4f5301e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_labels = NB.predict(X_test)\n",
    "confusion_table(nb_labels , L_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a7ddb6-e9c6-47f8-8604-c6905cf15037",
   "metadata": {},
   "source": [
    "Naive Bayes performs well on these data, with accurate predictions over 59% of the time. This is slightly worse than QDA, but much better than LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c737e-081e-4136-a462-09b68772eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB.predict_proba(X_test)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cffd3a-adc8-4c30-bce3-6b49b8d92d63",
   "metadata": {},
   "source": [
    "## *KNN - K-Nearest Neighbors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9fb371-6ee6-48fa-9ab6-3166edc9eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn1 = KNeighborsClassifier(n_neighbors =1)\n",
    "knn1.fit(X_train , L_train)\n",
    "knn1_pred = knn1.predict(X_test)\n",
    "confusion_table(knn1_pred , L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c417b1d-d7d6-4fed-b2ae-823b8e98842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(83+43)/252, np.mean(knn1_pred == L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc774611-9b4d-4b8a-ae17-b0c816d79185",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn3 = KNeighborsClassifier(n_neighbors =3)\n",
    "knn3_pred = knn3.fit(X_train , L_train).predict(X_test)\n",
    "np.mean(knn3_pred == L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0063475-89ef-4227-a96f-8fd6c5966703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766172b-3e6e-4af1-aa1f-f21ee69f3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Assumindo que você já tem: X (Lag1, Lag2) e y = Direction (Up/Down) ---\n",
    "# Ex.: X = df[['Lag1','Lag2']]\n",
    "#      y = df['Direction']  # dtype category (Up/Down)\n",
    "\n",
    "# 1) Split temporal (ex.: últimos 200 para teste)\n",
    "#n_test = 200\n",
    "#X_train, X_test = X.iloc[:-n_test], X.iloc[-n_test:]\n",
    "#y_train, y_test = y.iloc[:-n_test], y.iloc[-n_test:]\n",
    "\n",
    "# 2) Treinar KNN(3) e prever no teste\n",
    "#knn3 = KNeighborsClassifier(n_neighbors=3)\n",
    "y_pred = knn3.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# 3) Remontar DataFrame de previsões com índices originais\n",
    "idx_test = y_test.index\n",
    "pred_df = pd.DataFrame({\n",
    "    'idx':   idx_test,\n",
    "    'Lag1':  X_test['Lag1'].values,\n",
    "    'Lag2':  X_test['Lag2'].values,\n",
    "    'y_true': y_test.astype(str).values,\n",
    "    'y_pred': pd.Series(y_pred).astype(str).values\n",
    "}).set_index('idx')\n",
    "\n",
    "# 4) Print rápido (primeiras linhas)\n",
    "pred_view = pred_df.assign(\n",
    "    correct = (pred_df['y_true'] == pred_df['y_pred'])\n",
    ")\n",
    "print(pred_view.head(10))   # remova .head(10) para ver tudo\n",
    "\n",
    "# 5) Métricas\n",
    "acc = np.mean(pred_df['y_true'] == pred_df['y_pred'])\n",
    "print(f\"Acurácia KNN(3) no teste: {acc:.3f}\")\n",
    "\n",
    "print(\"\\nMatriz de confusão:\")\n",
    "print(confusion_matrix(pred_df['y_true'], pred_df['y_pred'], labels=['True','False']))\n",
    "\n",
    "print(\"\\nRelatório de classificação:\")\n",
    "print(classification_report(pred_df['y_true'], pred_df['y_pred'], target_names=['True','False']))\n",
    "\n",
    "# 6) Plot da “evolução” de direções (Up/Down) ao longo do índice de teste\n",
    "#    Como não temos preços, codificamos Up=+1, Down=-1 e mostramos acertos/erros.\n",
    "# map_dir = {'Up': 1, 'up': 1, 'DOWN': -1, 'Down': -1, 'down': -1}\n",
    "map_dir = {'True': 1, 'False': -1}\n",
    "y_true_num = pd.Series(pred_df['y_true']).map(map_dir)\n",
    "y_pred_num = pd.Series(pred_df['y_pred']).map(map_dir)\n",
    "\n",
    "t = np.arange(len(pred_df))  # eixo x sequencial no período de teste\n",
    "ok = (y_true_num == y_pred_num)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 3.5))\n",
    "# verdade (linha fina)\n",
    "ax.plot(t, y_true_num.values, linewidth=1, label='Direção real (+1=Up, -1=Down)')\n",
    "# acertos (marcadores sólidos)\n",
    "ax.scatter(t[ok & (y_pred_num==1)],  y_pred_num[ok & (y_pred_num==1)],  marker='^', s=40, label='Acerto Up')\n",
    "ax.scatter(t[ok & (y_pred_num==-1)], y_pred_num[ok & (y_pred_num==-1)], marker='v', s=40, label='Acerto Down')\n",
    "# erros (marcadores vazados)\n",
    "ax.scatter(t[~ok & (y_pred_num==1)],  y_pred_num[~ok & (y_pred_num==1)],  marker='^', s=80, facecolors='none', label='Erro Up')\n",
    "ax.scatter(t[~ok & (y_pred_num==-1)], y_pred_num[~ok & (y_pred_num==-1)], marker='v', s=80, facecolors='none', label='Erro Down')\n",
    "\n",
    "ax.set_yticks([-1, 1])\n",
    "ax.set_yticklabels(['Down (-1)', 'Up (+1)'])\n",
    "ax.set_xlabel('Instante no conjunto de teste (ordem temporal)')\n",
    "ax.set_title('Sequência de direções com previsões KNN(3)')\n",
    "ax.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ccd6b-cdb7-40a1-911c-53051730c3fc",
   "metadata": {},
   "source": [
    "## *Conceptual Exercises*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c21488-26e0-4c3a-9bd3-14671d1737ef",
   "metadata": {},
   "source": [
    "### (6) Suppose we collect data for a group of students in a statistics class with variables $X1$ = hours studied, $X2$ = undergrad GPA, and $Y$ = receive an A. We fit a logistic regression and produce estimated coefficient, $\\hat{\\beta}_0=-6$, $\\hat{\\beta}_1=0.05$, $\\hat{\\beta}_2=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9337a7-914c-4628-8022-725d3277ca2c",
   "metadata": {},
   "source": [
    "### (a) Estimate the probability that a student who studies for $40$h and has an undergrad GPA of $3.5$ gets an A in the class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57dca83-cf31-4c45-b2ae-76dd05daf58e",
   "metadata": {},
   "source": [
    "**Solution**: We fitted a logistic regression model for the probability of receiving an **A**:\n",
    "\n",
    "$$\n",
    "\\Pr(Y=1\\mid X_1, X_2)= \\frac{1}{1+\\exp(-\\eta)},\\quad \n",
    "\\eta=\\hat\\beta_0+\\hat\\beta_1 X_1+\\hat\\beta_2 X_2,\n",
    "$$\n",
    "with estimated coefficients $\\hat\\beta_0=-6$, $\\hat\\beta_1=0.05$ (hours studied $X_1$), and $\\hat\\beta_2=1$ (undergrad GPA $X_2$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b091316-0bf6-47b4-9c54-73b826f8fc95",
   "metadata": {},
   "source": [
    "$$\n",
    "\\eta = -6 + 0.05(40) + 1(3.5) = -6 + 2 + 3.5 = -0.5.\n",
    "$$\n",
    "$$\n",
    "\\Pr(Y=1\\mid X_1=40, X_2=3.5)=\\frac{1}{1+\\exp(0.5)}\\approx 0.378.\n",
    "$$\n",
    "\n",
    "$\\implies$ **Probability of getting an A = ~0.38**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8ea64-c3da-4976-ab5a-e6e2c566559b",
   "metadata": {},
   "source": [
    "### (b) How many hours would the student in part (a) need to study to have a $50\\%$ chance of getting an A in the class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37876ce-37ad-4c74-aadf-f747bd544ab1",
   "metadata": {},
   "source": [
    "A probability of $0.5$ occurs when $\\eta=0$. \n",
    "\n",
    "Now solving for $h$ (no. of study hours):\n",
    "\n",
    "$$\n",
    "0 = -6 + 0.05h + 1(3.5) \\quad\\Rightarrow\\quad 0.05h = 2.5 \\quad\\Rightarrow\\quad h=50.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6155e79-a529-48f9-8049-7748db14b95b",
   "metadata": {},
   "source": [
    "### (7) Suppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on X, last year’s percent profit. We examine a large number of companies and discover that the mean value of $X$ for companies that issued a dividend was $\\bar{X} = 10$, while the mean for those that didn’t was $\\bar{X} = 0$. In addition, the variance of $X$ for these two sets of companies was $\\hat\\sigma^2=36$. Finally, $80\\%$ of companies issued dividends. Assuming that $X$ follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was $X = 4$ last year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4b66e9-0c92-413d-80f2-91223f3af7a0",
   "metadata": {},
   "source": [
    "**Gaussian Naive Bayes with Bayes’ Theorem**\n",
    "\n",
    "Let $Y\\in\\{\\text{Yes},\\text{No}\\}$ indicate issuing a dividend.  \n",
    "\n",
    "Given:\n",
    "- Priors: $P(Y=\\text{Yes})=0.8,\\; P(Y=\\text{No})=0.2$.\n",
    "- Class-conditional $X\\mid Y\\sim \\mathcal N(\\mu_Y,\\sigma^2)$ with common variance $\\sigma^2=36$ ($\\sigma=6$).\n",
    "  - $\\mu_{\\text{Yes}}=10,\\; \\mu_{\\text{No}}=0$.\n",
    "\n",
    "For $x=4$, the normal density is\n",
    "$$\n",
    "f(x\\mid\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right).\n",
    "$$\n",
    "\n",
    "Computing the likelihoods (the prefactor cancels since $\\sigma$ is common):\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(4\\mid Y=\\text{Yes}) &\\propto \\exp\\!\\left(-\\frac{(4-10)^2}{72}\\right) = e^{-1/2},\\\\\n",
    "f(4\\mid Y=\\text{No})  &\\propto \\exp\\!\\left(-\\frac{(4-0)^2}{72}\\right) = e^{-2/9}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From Bayes’ theorem:\n",
    "$$\n",
    "P(Y=\\text{Yes}\\mid X=4)=\n",
    "\\frac{f(4\\mid \\text{Yes})P(\\text{Yes})}\n",
    "     {f(4\\mid \\text{Yes})P(\\text{Yes}) + f(4\\mid \\text{No})P(\\text{No}) }.\n",
    "$$\n",
    "\n",
    "Numerically:\n",
    "$$\n",
    "\\frac{0.8\\,e^{-1/2}}{0.8\\,e^{-1/2}+0.2\\,e^{-2/9}}\n",
    "=\\frac{0.8\\cdot 0.6065}{0.8\\cdot 0.6065 + 0.2\\cdot 0.8007}\n",
    "\\approx \\frac{0.4852}{0.6454}\n",
    "\\approx 0.75.\n",
    "$$\n",
    "\n",
    "**The probability the company will issue a dividend given $X=4$ is ≈ 0.75**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e330d5-54b4-4a17-9896-eb3f98c8ea39",
   "metadata": {},
   "source": [
    "## *Applies Exercises*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b45c67e-646b-497a-a56e-04a6b2025b81",
   "metadata": {},
   "source": [
    "### (13) This question should be answered using the `Weekly` data set, which is part of the ISLP package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains $1,089$ weekly returns for $21$ years, from the beginning of $1990$ to the end of $2010$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f46c42-93d1-44e6-898e-108ec3cd7918",
   "metadata": {},
   "source": [
    "### (a) Produce some numerical and graphical summaries of the `Weekly` data. Do there appear to be any patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78bc6fd-0791-4089-9cef-f6cba8475a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weekly = load_data('Weekly')\n",
    "Weekly.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a869f59-646e-4791-8883-0cf3812366ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---- setup ----\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da233f8-7343-4e4f-93e5-b7befbf572fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\")\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.4f}\")\n",
    "\n",
    "# df = Weekly   # <-- make sure your DataFrame is named `Weekly`\n",
    "df = Weekly.copy()\n",
    "\n",
    "# ---- 1) Numerical summaries ----\n",
    "display(df.describe(include=\"all\").T)\n",
    "\n",
    "# Class balance\n",
    "vc = df[\"Direction\"].value_counts()\n",
    "vc_norm = df[\"Direction\"].value_counts(normalize=True).rename(\"proportion\")\n",
    "display(pd.concat([vc.rename(\"count\"), vc_norm], axis=1))\n",
    "\n",
    "# Correlations among numeric columns\n",
    "corr = df.select_dtypes(include=[np.number]).corr()\n",
    "display(corr.style.background_gradient(cmap=\"coolwarm\", vmin=-1, vmax=1))\n",
    "\n",
    "# ---- 2) Graphical summaries ----\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 9))\n",
    "\n",
    "# (a) Returns today\n",
    "axes[0,0].hist(df[\"Today\"], bins=40, edgecolor=\"k\")\n",
    "axes[0,0].set_title(\"Histogram of Today (weekly return)\")\n",
    "axes[0,0].set_xlabel(\"Return\"); axes[0,0].set_ylabel(\"Count\")\n",
    "\n",
    "# (b) Volume over time\n",
    "df.groupby(\"Year\")[\"Volume\"].mean().plot(ax=axes[0,1])\n",
    "axes[0,1].set_title(\"Average Weekly Volume by Year\")\n",
    "axes[0,1].set_xlabel(\"Year\"); axes[0,1].set_ylabel(\"Avg Volume\")\n",
    "\n",
    "# (c) Today vs Lag1\n",
    "axes[1,0].scatter(df[\"Lag1\"], df[\"Today\"], s=12, alpha=0.6)\n",
    "axes[1,0].axhline(0, ls=\"--\", lw=1, c=\"grey\"); axes[1,0].axvline(0, ls=\"--\", lw=1, c=\"grey\")\n",
    "axes[1,0].set_title(\"Today vs Lag1\"); axes[1,0].set_xlabel(\"Lag1\"); axes[1,0].set_ylabel(\"Today\")\n",
    "\n",
    "# (d) Pairwise relationships (small sample for readability)\n",
    "sns.pairplot(\n",
    "    df.sample(min(400, len(df)), random_state=1),\n",
    "    vars=[\"Lag1\",\"Lag2\",\"Lag3\",\"Lag4\",\"Lag5\",\"Today\"],\n",
    "    corner=True, plot_kws=dict(s=12, alpha=0.5)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589978ca-17d1-42e1-85cf-bfafc6f7a11b",
   "metadata": {},
   "source": [
    "What patterns usually appear in this dataset\n",
    "\n",
    "- **Returns center near zero**. The weekly returns (`Today` and the lags) are roughly symmetric and concentrated around 0, with occasional outliers.\n",
    "\n",
    "- **Slight class imbalance**. `Direction` typically has slightly more *Up* than *Down* weeks (but not dramatically so).\n",
    "\n",
    "- **Weak linear signal from lags**. Correlations between Today and `Lag1`–`Lag5` are very small (often close to 0), suggesting limited linear predictive power from past week returns.\n",
    "\n",
    "- **Clear time trend in trading activity**. Volume increases over the years (plot of average `volume` by Year shows an upward trend), while returns do not show a comparable trend.\n",
    "\n",
    "- **No obvious low-dimensional structure**. Scatterplots among Lag variables and Today look diffuse, indicating weak simple relationships.\n",
    "\n",
    "> These observations motivate trying classification models but set expectations that simple linear relationships using only lagged returns may have limited predictive strength."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28383c-ff9d-48ff-aba6-f55c96f0351f",
   "metadata": {},
   "source": [
    "### (b) Use the full data set to perform a logistic regression with `Direction` as the response and the five lag variables plus `Volume` as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab21628-2b87-4d17-8df3-ae49db650011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Response as binary: 1 = \"Up\", 0 = \"Down\"\n",
    "df[\"Up\"] = (df[\"Direction\"].astype(str) == \"Up\").astype(int)\n",
    "\n",
    "# Fit logistic regression with all five lags + Volume\n",
    "model = smf.logit(\"Up ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume\", data=df)\n",
    "res = model.fit(disp=False)\n",
    "\n",
    "# Full regression table\n",
    "print(res.summary())\n",
    "\n",
    "# Identify statistically significant predictors at alpha=0.05\n",
    "alpha = 0.05\n",
    "sig = res.pvalues[res.pvalues < alpha].drop(\"Intercept\", errors=\"ignore\")\n",
    "print(\"\\nSignificant at α = 0.05:\")\n",
    "print(sig.sort_values())\n",
    "\n",
    "# (Optional) Odds ratios with 95% CIs\n",
    "params = res.params\n",
    "conf = res.conf_int()\n",
    "or_table = pd.DataFrame({\n",
    "    \"OR\": np.exp(params),\n",
    "    \"2.5%\": np.exp(conf[0]),\n",
    "    \"97.5%\": np.exp(conf[1]),\n",
    "    \"p-value\": res.pvalues\n",
    "})\n",
    "display(or_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0085cd-63a0-4095-8b89-4845a2f21959",
   "metadata": {},
   "source": [
    "**Interpretation** (expected with the Weekly data): only `Lag2` typically appears statistically significant at the 5% level (positive coefficient), while `Lag1`, `Lag3`, `Lag4`, `Lag5`, and `Volume` are not statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efba831-ead3-49d3-a00a-7d8b9634c4d8",
   "metadata": {},
   "source": [
    "### (c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac234f-cfdf-41d5-b4cb-4508c9a404a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc (cm):\n",
    "    acc = (y_pred == y_test).mean()\n",
    "    tp = cm.loc[\"Up\",\"Up\"]; tn = cm.loc[\"Down\",\"Down\"]\n",
    "    fp = cm.loc[\"Up\",\"Down\"]; fn = cm.loc[\"Down\",\"Up\"]\n",
    "    tpr = tp / (tp + fn) if (tp + fn) else np.nan  # sensitivity for 'Up'\n",
    "    tnr = tn / (tn + fp) if (tn + fp) else np.nan  # specificity for 'Down'\n",
    "\n",
    "    print(f\"Overall test accuracy (2009–2010): {acc:.3f}\")\n",
    "    print(f\"Sensitivity (Up): {tpr:.3f} | Specificity (Down): {tnr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30dcf25-7617-43ec-b882-6ce102165e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- fit (again, for a self-contained cell) ---\n",
    "df[\"Up\"] = (df[\"Direction\"].astype(str) == \"Up\").astype(int)\n",
    "\n",
    "logit = smf.logit(\"Up ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume\", data=df).fit(disp=False)\n",
    "\n",
    "# --- in-sample predictions, default 0.5 threshold ---\n",
    "probs = logit.predict(df)\n",
    "pred = pd.Series(np.where(probs >= 0.5, \"Up\", \"Down\"), index=df.index, name=\"Predicted\")\n",
    "actual = df[\"Direction\"].astype(str)\n",
    "\n",
    "# confusion matrix with fixed order\n",
    "labels = [\"Down\", \"Up\"]\n",
    "cm_log = pd.crosstab(pred, actual).reindex(index=labels, columns=labels, fill_value=0)\n",
    "display(cm_log)\n",
    "\n",
    "# accuracy and a couple of helpful rates\n",
    "calc_acc (cm_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9045564d-41aa-4bc2-b206-94d42bd121ae",
   "metadata": {},
   "source": [
    "### (d) Now fit the logistic regression model using a training data period from $1990$ to $2008$, with `Lag2` as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from $2009$ and $2010$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbd7313-bded-4d4e-ac2d-ac1845469535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: 1990–2008 | Test: 2009–2010\n",
    "train_idx = df[\"Year\"] <= 2008\n",
    "test_idx  = df[\"Year\"] >= 2009\n",
    "\n",
    "# Fit logistic regression with Lag2 only (on training period)\n",
    "logit_lag2 = smf.logit(\"Up ~ Lag2\", data=df.loc[train_idx]).fit(disp=False)\n",
    "\n",
    "# Predict on held-out 2009–2010\n",
    "probs_test = logit_lag2.predict(df.loc[test_idx])\n",
    "pred_test  = np.where(probs_test >= 0.5, \"Up\", \"Down\")\n",
    "\n",
    "actual_test = df.loc[test_idx, \"Direction\"].astype(str)\n",
    "\n",
    "# Confusion matrix (rows = predicted, cols = actual)\n",
    "labels = [\"Down\", \"Up\"]\n",
    "cm_log_l2 = pd.crosstab(\n",
    "    pd.Series(pred_test, index=actual_test.index, name=\"Predicted\"),\n",
    "    actual_test,\n",
    ").reindex(index=labels, columns=labels, fill_value=0)\n",
    "display(cm_log_l2)\n",
    "\n",
    "# Overall accuracy (fraction correct)\n",
    "calc_acc (cm_log_l2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e610ae-fd89-4897-ba82-e2a76c95e279",
   "metadata": {},
   "source": [
    "### (e) Repeat (d) using LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c05a02-4b69-41ca-a066-efa595208a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Data\n",
    "df[\"Direction\"] = df[\"Direction\"].astype(str)\n",
    "\n",
    "# Train/Test split\n",
    "train_idx = df[\"Year\"] <= 2008\n",
    "test_idx  = df[\"Year\"] >= 2009\n",
    "\n",
    "X_train = df.loc[train_idx, [\"Lag2\"]].to_numpy()\n",
    "y_train = df.loc[train_idx, \"Direction\"].to_numpy()\n",
    "\n",
    "X_test  = df.loc[test_idx, [\"Lag2\"]].to_numpy()\n",
    "y_test  = df.loc[test_idx, \"Direction\"].to_numpy()\n",
    "\n",
    "# Fit LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test period (2009–2010)\n",
    "y_pred = lda.predict(X_test)\n",
    "\n",
    "# Confusion matrix (rows = predicted, cols = actual)\n",
    "labels = [\"Down\", \"Up\"]\n",
    "cm_lda = pd.crosstab(\n",
    "    pd.Series(y_pred, index=df.index[test_idx], name=\"Predicted\"),\n",
    "    pd.Series(y_test, index=df.index[test_idx], name=\"Actual\")\n",
    ").reindex(index=labels, columns=labels, fill_value=0)\n",
    "display(cm_lda)\n",
    "\n",
    "# Overall accuracy and a couple of rates\n",
    "calc_acc (cm_lda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1483e1e5-aa76-40b8-a12f-9058f948e4c7",
   "metadata": {},
   "source": [
    "### (f) Repeat (d) using QDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb065d-8126-4065-8622-635a41b0faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# Train/Test split\n",
    "train_idx = df[\"Year\"] <= 2008\n",
    "test_idx  = df[\"Year\"] >= 2009\n",
    "\n",
    "X_train = df.loc[train_idx, [\"Lag2\"]].to_numpy()\n",
    "y_train = df.loc[train_idx, \"Direction\"].to_numpy()\n",
    "\n",
    "X_test  = df.loc[test_idx, [\"Lag2\"]].to_numpy()\n",
    "y_test  = df.loc[test_idx, \"Direction\"].to_numpy()\n",
    "\n",
    "# Fit QDA\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set (2009–2010)\n",
    "y_pred = qda.predict(X_test)\n",
    "\n",
    "# Confusion matrix (rows = predicted, cols = actual)\n",
    "labels = [\"Down\", \"Up\"]\n",
    "cm_qda = pd.crosstab(\n",
    "    pd.Series(y_pred, index=df.index[test_idx], name=\"Predicted\"),\n",
    "    pd.Series(y_test, index=df.index[test_idx], name=\"Actual\")\n",
    ").reindex(index=labels, columns=labels, fill_value=0)\n",
    "display(cm_qda)\n",
    "\n",
    "# Overall accuracy and helpful rates\n",
    "calc_acc (cm_qda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce938f5a-9c3b-4400-8a9f-09a65d78f90c",
   "metadata": {},
   "source": [
    "### Repeat (g) using KNN with K = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c24e2f-0da3-402a-afcf-7c0a9fdf354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train/Test split\n",
    "train_idx = df[\"Year\"] <= 2008\n",
    "test_idx  = df[\"Year\"] >= 2009\n",
    "\n",
    "X_train = df.loc[train_idx, [\"Lag2\"]].to_numpy()\n",
    "y_train = df.loc[train_idx, \"Direction\"].to_numpy()\n",
    "\n",
    "X_test  = df.loc[test_idx, [\"Lag2\"]].to_numpy()\n",
    "y_test  = df.loc[test_idx, \"Direction\"].to_numpy()\n",
    "\n",
    "# Fit KNN with K=1\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test period (2009–2010)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Confusion matrix (rows = predicted, cols = actual)\n",
    "labels = [\"Down\", \"Up\"]\n",
    "cm_knn = pd.crosstab(\n",
    "    pd.Series(y_pred, index=df.index[test_idx], name=\"Predicted\"),\n",
    "    pd.Series(y_test, index=df.index[test_idx], name=\"Actual\")\n",
    ").reindex(index=labels, columns=labels, fill_value=0)\n",
    "display(cm_knn)\n",
    "\n",
    "# Overall accuracy and helpful rates\n",
    "calc_acc (cm_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3300e-c846-4376-bf48-032541df9f0a",
   "metadata": {},
   "source": [
    "### (h) Repeat (d) using naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d8bd1-f5cc-42e2-9f1a-a350b4703c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train/Test split\n",
    "train_idx = df[\"Year\"] <= 2008\n",
    "test_idx  = df[\"Year\"] >= 2009\n",
    "\n",
    "X_train = df.loc[train_idx, [\"Lag2\"]].to_numpy()\n",
    "y_train = df.loc[train_idx, \"Direction\"].to_numpy()\n",
    "\n",
    "X_test  = df.loc[test_idx, [\"Lag2\"]].to_numpy()\n",
    "y_test  = df.loc[test_idx, \"Direction\"].to_numpy()\n",
    "\n",
    "# Fit Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test (2009–2010)\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Confusion matrix (rows = predicted, cols = actual)\n",
    "labels = [\"Down\", \"Up\"]\n",
    "cm_nb = pd.crosstab(\n",
    "    pd.Series(y_pred, index=df.index[test_idx], name=\"Predicted\"),\n",
    "    pd.Series(y_test, index=df.index[test_idx], name=\"Actual\")\n",
    ").reindex(index=labels, columns=labels, fill_value=0)\n",
    "display(cm_nb)\n",
    "\n",
    "# Overall accuracy + a couple of helpful rates\n",
    "calc_acc (cm_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e34698-9f25-46e1-97d3-cbd092b0f9d9",
   "metadata": {},
   "source": [
    "### (i) Which of these methods appears to provide the best results on this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc0d8e-1823-4bc4-b29d-f89bbba601e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==> logit\\n\")\n",
    "calc_acc(cm_log)\n",
    "print(\"\\n==> logit + lag2\\n\")\n",
    "calc_acc(cm_log_l2)\n",
    "print(\"\\n==> lda\\n\")\n",
    "calc_acc(cm_lda)\n",
    "print(\"\\n==> qda\\n\")\n",
    "calc_acc(cm_qda)\n",
    "print(\"\\n==> knn, k=1\\n\")\n",
    "calc_acc(cm_knn)\n",
    "print(\"\\n==> Naive Bayes\\n\")\n",
    "calc_acc(cm_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b950796-2abc-419c-be67-c48366dc1d3e",
   "metadata": {},
   "source": [
    "**Short answer:** By plain accuracy, **all models tie at 0.587**, which equals the “always predict Up” baseline — so none truly “wins” on accuracy.\n",
    "\n",
    "**A better tie-breaker:** use **balanced accuracy** = (Sensitivity + Specificity)/2  \n",
    "(combats the skew toward predicting *Up*).\n",
    "\n",
    "| Model               | Sensitivity | Specificity | Balanced Acc. |\n",
    "|---------------------|-------------|-------------|----------------|\n",
    "| Logit (all vars)    | 0.921       | 0.112       | **0.517**      |\n",
    "| Logit (Lag2 only)   | 0.918       | 0.209       | **0.564** 🔹   |\n",
    "| **LDA (Lag2 only)** | 0.918       | 0.209       | **0.564** 🔹   |\n",
    "| QDA (Lag2 only)     | 1.000       | 0.000       | 0.500          |\n",
    "| KNN, K=1 (Lag2)     | 0.492       | 0.512       | 0.502          |\n",
    "| Naive Bayes (Lag2)  | 1.000       | 0.000       | 0.500          |\n",
    "\n",
    "**Verdict:** **LDA (Lag2)** and **Logit (Lag2)** are the best of this set (tied), offering the highest balanced accuracy and a modest boost in specificity while keeping high sensitivity.  \n",
    "**Note:** Since overall accuracy matches the majority-class rate, these models have limited practical predictiveness on this test; consider threshold tuning, adding features, or alternative models (e.g., trees/ensembles) and time-series CV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3e3e6-397d-4176-bb1d-20977a163799",
   "metadata": {},
   "source": [
    "### (j) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd85045-e0a6-4174-b23c-2afd1c7cc50e",
   "metadata": {},
   "source": [
    "> Train on **1990–2008**, test on **2009–2010**.  \n",
    "> We try multiple predictor sets, simple transforms/interactions, and a K grid for KNN.  \n",
    "> We report per-method winners and the **overall best** by **balanced accuracy** $(\\text{TPR}+\\text{TNR})/2$, along with the confusion matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d1408-336e-4044-9484-8f465b14acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4396b2-3a57-4e1a-8d3b-6251e4a74c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Data prep\n",
    "# ---------------------------\n",
    "df = Weekly.copy()\n",
    "df = df.assign(\n",
    "    Direction=df[\"Direction\"].astype(str),\n",
    "    Up=(df[\"Direction\"].astype(str)==\"Up\").astype(int),\n",
    "    Lag2_sq=df[\"Lag2\"]**2,\n",
    "    Lag1xLag2=df[\"Lag1\"]*df[\"Lag2\"],\n",
    "    logV=np.log(df[\"Volume\"].clip(lower=1e-12)),\n",
    "    Lag2_pos=(df[\"Lag2\"]>0).astype(int)\n",
    ")\n",
    "\n",
    "train_idx = df[\"Year\"] <= 2008\n",
    "test_idx  = df[\"Year\"] >= 2009\n",
    "\n",
    "y_train = df.loc[train_idx, \"Direction\"].to_numpy()\n",
    "y_test  = df.loc[test_idx,  \"Direction\"].to_numpy()\n",
    "y_train_bin = (y_train == \"Up\").astype(int)\n",
    "y_test_bin  = (y_test  == \"Up\").astype(int)\n",
    "\n",
    "# Candidate feature sets\n",
    "feature_sets = {\n",
    "    \"Lag2\"              : [\"Lag2\"],\n",
    "    \"Lag2 + Lag1\"       : [\"Lag2\",\"Lag1\"],\n",
    "    \"Lag2 + Lag2^2\"     : [\"Lag2\",\"Lag2_sq\"],\n",
    "    \"Lag2 + Lag1:Lag2\"  : [\"Lag2\",\"Lag1\",\"Lag1xLag2\"],\n",
    "    \"Lags1-5\"           : [\"Lag1\",\"Lag2\",\"Lag3\",\"Lag4\",\"Lag5\"],\n",
    "    \"Lags1-5 + logV\"    : [\"Lag1\",\"Lag2\",\"Lag3\",\"Lag4\",\"Lag5\",\"logV\"],\n",
    "    \"Lag2 + logV\"       : [\"Lag2\",\"logV\"],\n",
    "    \"Lag2_pos (indicator)\": [\"Lag2_pos\"],        # step function on Lag2\n",
    "}\n",
    "\n",
    "# Helper to compute metrics from a confusion matrix\n",
    "def summarize(cm, labels=(\"Down\",\"Up\")):\n",
    "    # cm rows = pred, cols = actual, order by labels\n",
    "    tn = cm[0,0]; fp = cm[0,1]\n",
    "    fn = cm[1,0]; tp = cm[1,1]\n",
    "    total = cm.sum()\n",
    "    acc = (tp + tn) / total if total else np.nan\n",
    "    tpr = tp / (tp + fn) if (tp + fn) else np.nan   # sensitivity for Up\n",
    "    tnr = tn / (tn + fp) if (tn + fp) else np.nan   # specificity for Down\n",
    "    bal = 0.5*(tpr + tnr) if np.isfinite(tpr) and np.isfinite(tnr) else np.nan\n",
    "    return dict(acc=acc, tpr=tpr, tnr=tnr, bal=bal, tp=tp, tn=tn, fp=fp, fn=fn)\n",
    "\n",
    "def confmat_df(cm, labels=(\"Down\",\"Up\")):\n",
    "    return pd.DataFrame(cm, index=pd.Index(labels, name=\"Predicted\"), columns=pd.Index(labels, name=\"Actual\"))\n",
    "\n",
    "# Containers for best per method\n",
    "best = {}   # method -> dict with keys: name, Xcols, cm, metrics, extra (e.g., K)\n",
    "overall_best = None\n",
    "\n",
    "# ---------------------------\n",
    "# 2) LOGISTIC REGRESSION (GLM Binomial with logit link)\n",
    "# ---------------------------\n",
    "for name, cols in feature_sets.items():\n",
    "    X_train = df.loc[train_idx, cols]\n",
    "    X_test  = df.loc[test_idx,  cols]\n",
    "\n",
    "    X_train_sm = sm.add_constant(X_train)\n",
    "    X_test_sm  = sm.add_constant(X_test)\n",
    "\n",
    "    # Fit GLM Binomial (more stable than Logit for edge cases)\n",
    "    glm = sm.GLM(y_train_bin, X_train_sm, family=sm.families.Binomial())\n",
    "    res = glm.fit()\n",
    "\n",
    "    # Predict with default 0.5 threshold\n",
    "    p_test = res.predict(X_test_sm)\n",
    "    y_pred = np.where(p_test >= 0.5, \"Up\", \"Down\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[\"Down\",\"Up\"])\n",
    "    metrics = summarize(cm)\n",
    "    rec = dict(method=\"Logit\", name=name, Xcols=cols, cm=cm, metrics=metrics)\n",
    "\n",
    "    if \"Logit\" not in best or metrics[\"bal\"] > best[\"Logit\"][\"metrics\"][\"bal\"]:\n",
    "        best[\"Logit\"] = rec\n",
    "\n",
    "# ---------------------------\n",
    "# 3) LDA\n",
    "# ---------------------------\n",
    "for name, cols in feature_sets.items():\n",
    "    X_train = df.loc[train_idx, cols].to_numpy()\n",
    "    X_test  = df.loc[test_idx,  cols].to_numpy()\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X_train, y_train)\n",
    "    y_pred = lda.predict(X_test)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[\"Down\",\"Up\"])\n",
    "    metrics = summarize(cm)\n",
    "    rec = dict(method=\"LDA\", name=name, Xcols=cols, cm=cm, metrics=metrics)\n",
    "\n",
    "    if \"LDA\" not in best or metrics[\"bal\"] > best[\"LDA\"][\"metrics\"][\"bal\"]:\n",
    "        best[\"LDA\"] = rec\n",
    "\n",
    "# ---------------------------\n",
    "# 4) QDA\n",
    "# ---------------------------\n",
    "for name, cols in feature_sets.items():\n",
    "    X_train = df.loc[train_idx, cols].to_numpy()\n",
    "    X_test  = df.loc[test_idx,  cols].to_numpy()\n",
    "    qda = QuadraticDiscriminantAnalysis()\n",
    "    # QDA can fail if a class has 0 variance along a feature; guard with try\n",
    "    try:\n",
    "        qda.fit(X_train, y_train)\n",
    "        y_pred = qda.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=[\"Down\",\"Up\"])\n",
    "        metrics = summarize(cm)\n",
    "        rec = dict(method=\"QDA\", name=name, Xcols=cols, cm=cm, metrics=metrics)\n",
    "        if \"QDA\" not in best or metrics[\"bal\"] > best[\"QDA\"][\"metrics\"][\"bal\"]:\n",
    "            best[\"QDA\"] = rec\n",
    "    except Exception as e:\n",
    "        pass  # skip problematic specs\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Gaussian Naive Bayes\n",
    "# ---------------------------\n",
    "for name, cols in feature_sets.items():\n",
    "    X_train = df.loc[train_idx, cols].to_numpy()\n",
    "    X_test  = df.loc[test_idx,  cols].to_numpy()\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    y_pred = gnb.predict(X_test)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[\"Down\",\"Up\"])\n",
    "    metrics = summarize(cm)\n",
    "    rec = dict(method=\"NaiveBayes\", name=name, Xcols=cols, cm=cm, metrics=metrics)\n",
    "\n",
    "    if \"NaiveBayes\" not in best or metrics[\"bal\"] > best[\"NaiveBayes\"][\"metrics\"][\"bal\"]:\n",
    "        best[\"NaiveBayes\"] = rec\n",
    "\n",
    "# ---------------------------\n",
    "# 6) KNN (with scaling) — search K\n",
    "# ---------------------------\n",
    "K_grid = [1, 3, 5, 7, 9, 11, 15, 21, 31]\n",
    "\n",
    "for name, cols in feature_sets.items():\n",
    "    X_train = df.loc[train_idx, cols].to_numpy()\n",
    "    X_test  = df.loc[test_idx,  cols].to_numpy()\n",
    "\n",
    "    for K in K_grid:\n",
    "        knn = make_pipeline(StandardScaler(with_mean=True, with_std=True), KNeighborsClassifier(n_neighbors=K))\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=[\"Down\",\"Up\"])\n",
    "        metrics = summarize(cm)\n",
    "        rec = dict(method=\"KNN\", name=f\"{name} | K={K}\", Xcols=cols, cm=cm, metrics=metrics, K=K)\n",
    "\n",
    "        if \"KNN\" not in best or metrics[\"bal\"] > best[\"KNN\"][\"metrics\"][\"bal\"]:\n",
    "            best[\"KNN\"] = rec\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Display per-method best and pick overall best\n",
    "# ---------------------------\n",
    "rows = []\n",
    "for method, rec in best.items():\n",
    "    m = rec[\"metrics\"]\n",
    "    rows.append([\n",
    "        method,\n",
    "        rec[\"name\"],\n",
    "        f\"{m['acc']:.3f}\",\n",
    "        f\"{m['tpr']:.3f}\",\n",
    "        f\"{m['tnr']:.3f}\",\n",
    "        f\"{m['bal']:.3f}\"\n",
    "    ])\n",
    "    if (overall_best is None) or (m[\"bal\"] > overall_best[\"metrics\"][\"bal\"]):\n",
    "        overall_best = rec\n",
    "\n",
    "summary_df = pd.DataFrame(rows, columns=[\"Method\",\"Best feature set / K\",\"Accuracy\",\"Sensitivity (Up)\",\"Specificity (Down)\",\"Balanced Acc.\"])\n",
    "display(summary_df.sort_values(\"Balanced Acc.\", ascending=False, key=lambda s: s.astype(float)))\n",
    "\n",
    "print(\"\\n🏆 Best overall by Balanced Accuracy:\")\n",
    "m = overall_best[\"metrics\"]\n",
    "print(f\"- Method: {overall_best['method']}\")\n",
    "print(f\"- Features: {overall_best['name']} -> {overall_best['Xcols']}\")\n",
    "print(f\"- Accuracy: {m['acc']:.3f} | Sensitivity (Up): {m['tpr']:.3f} | Specificity (Down): {m['tnr']:.3f} | Balanced Acc.: {m['bal']:.3f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix (rows=Pred, cols=Actual):\")\n",
    "display(confmat_df(overall_best[\"cm\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
